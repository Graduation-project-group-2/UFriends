{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5892ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e21384c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"skt/kogpt2-base-v2\",\n",
    "  bos_token='</s>', eos_token='</s>', unk_token='<unk>',\n",
    "  pad_token='<pad>', mask_token='<mask>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64108a99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['â–ì•ˆë…•',\n",
       " 'í•˜',\n",
       " 'ì„¸',\n",
       " 'ìš”.',\n",
       " 'â–í•œêµ­ì–´',\n",
       " 'â–G',\n",
       " 'P',\n",
       " 'T',\n",
       " '-2',\n",
       " 'â–ì…',\n",
       " 'ë‹ˆë‹¤.',\n",
       " 'ğŸ˜¤',\n",
       " ':)',\n",
       " 'l^o']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"ì•ˆë…•í•˜ì„¸ìš”. í•œêµ­ì–´ GPT-2 ì…ë‹ˆë‹¤.ğŸ˜¤:)l^o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "777950cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d13d3aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21fb80176fc44a9ca4d1fa74cde43180",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/490M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')\n",
    "text = 'ê·¼ìœ¡ì´ ì»¤ì§€ê¸° ìœ„í•´ì„œëŠ”'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d99aa4f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ê·¼ìœ¡ì´ ì»¤ì§€ê¸° ìœ„í•´ì„œëŠ” ë¬´ì—‡ë³´ë‹¤ ê·œì¹™ì ì¸ ìƒí™œìŠµê´€ì´ ì¤‘ìš”í•˜ë‹¤.\n",
      "íŠ¹íˆ, ì•„ì¹¨ì‹ì‚¬ëŠ” ë‹¨ë°±ì§ˆê³¼ ë¹„íƒ€ë¯¼ì´ í’ë¶€í•œ ê³¼ì¼ê³¼ ì±„ì†Œë¥¼ ë§ì´ ì„­ì·¨í•˜ëŠ” ê²ƒì´ ì¢‹ë‹¤.\n",
      "ë˜í•œ í•˜ë£¨ 30ë¶„ ì´ìƒ ì¶©ë¶„í•œ ìˆ˜ë©´ì„ ì·¨í•˜ëŠ” ê²ƒë„ ë„ì›€ì´ ëœë‹¤.\n",
      "ì•„ì¹¨ ì‹ì‚¬ë¥¼ ê±°ë¥´ì§€ ì•Šê³  ê·œì¹™ì ìœ¼ë¡œ ìš´ë™ì„ í•˜ë©´ í˜ˆì•¡ìˆœí™˜ì— ë„ì›€ì„ ì¤„ ë¿ë§Œ ì•„ë‹ˆë¼ ì‹ ì§„ëŒ€ì‚¬ë¥¼ ì´‰ì§„í•´ ì²´ë‚´ ë…¸íë¬¼ì„ ë°°ì¶œí•˜ê³  í˜ˆì••ì„ ë‚®ì¶°ì¤€ë‹¤.\n",
      "ìš´ë™ì€ í•˜ë£¨ì— 10ë¶„ ì •ë„ë§Œ í•˜ëŠ” ê²Œ ì¢‹ìœ¼ë©° ìš´ë™ í›„ì—ëŠ” ë°˜ë“œì‹œ ìŠ¤íŠ¸ë ˆì¹­ì„ í†µí•´ ê·¼ìœ¡ëŸ‰ì„ ëŠ˜ë¦¬ê³  ìœ ì—°ì„±ì„ ë†’ì—¬ì•¼ í•œë‹¤.\n",
      "ìš´ë™ í›„ ë°”ë¡œ ì ìë¦¬ì— ë“œëŠ” ê²ƒì€ í”¼í•´ì•¼ í•˜ë©° íŠ¹íˆ ì•„ì¹¨ì— ì¼ì–´ë‚˜ë©´ ëª¸ì´ í”¼ê³¤í•´ì§€ê¸° ë•Œë¬¸ì— ë¬´ë¦¬í•˜ê²Œ ì›€ì§ì´ë©´ ì˜¤íˆë ¤ ì—­íš¨ê³¼ê°€ ë‚  ìˆ˜ë„ ìˆë‹¤.\n",
      "ìš´ë™ì„\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.encode(text, return_tensors='pt')\n",
    "gen_ids = model.generate(input_ids,\n",
    "                           max_length=128,\n",
    "                           repetition_penalty=2.0,\n",
    "                           pad_token_id=tokenizer.pad_token_id,\n",
    "                           eos_token_id=tokenizer.eos_token_id,\n",
    "                           bos_token_id=tokenizer.bos_token_id,\n",
    "                           use_cache=True)\n",
    "\n",
    "generated = tokenizer.decode(gen_ids[0])\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3d027cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í˜„ëŒ€ì¸ë“¤ì€ ì™œ í•­ìƒ ë¶ˆì•ˆí•´ í• ê¹Œ?\"\n",
      "\"ê·¸ë ‡ë‹¤ë©´ ê·¸ê±´ ë°”ë¡œ ìš°ë¦¬ë“¤ì˜ ë¬¸ì œì…ë‹ˆë‹¤. ìš°ë¦¬ê°€ ì§€ê¸ˆ ì´ ìˆœê°„ì—ë„, ê·¸ë¦¬ê³  ì•ìœ¼ë¡œë„ ê³„ì† ê±±ì •í•˜ê³  ìˆëŠ” ê²ƒì€ ìš°ë¦¬ì˜ ë¯¸ë˜ê°€ ë¶ˆíˆ¬ëª…í•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤! ìš°ë¦¬ëŠ” ì´ë¯¸ ì˜¤ë˜ ì „ë¶€í„° ë¯¸ë˜ì— ëŒ€í•œ ê±±ì •ì„ í•˜ê³  ìˆìŠµë‹ˆë‹¤. ê·¸ë˜ì„œ ì €ëŠ” ì˜¤ëŠ˜ë¶€í„°ë¼ë„ ë¯¸ë˜ë¥¼ ìœ„í•´ ë¬´ì—‡ì„ í•´ì•¼ í• ì§€ ê³ ë¯¼í•´ì•¼ í•©ë‹ˆë‹¤. ê·¸ëŸ¬ê¸° ìœ„í•´ì„œëŠ” ë¨¼ì € í˜„ì¬ì˜ ìƒí™©ì„ ëƒ‰ì •í•˜ê²Œ ë¶„ì„í•˜ê³  ê·¸ì— ë§ëŠ” ëŒ€ì±…ì„ ì„¸ì›Œì•¼ í•©ë‹ˆë‹¤.\n",
      "ìš°ë¦¬ê°€ í˜„ì¬ ì§ë©´í•œ ê°€ì¥ í° ë¬¸ì œëŠ” 'ìš°ë¦¬ë“¤'ì´ ê³¼ì—° ì–´ë–¤ ìƒê°ì„ ê°€ì§€ê³  ìˆëŠ”ì§€ ëª¨ë¥¸ë‹¤ëŠ” ì ì…ë‹ˆë‹¤.\n",
      "ì´ëŸ° ìƒí™©ì—ì„œ ì–´ë–»ê²Œ ëŒ€ì²˜í•  ê²ƒì¸ê°€ì— ëŒ€í•´ ìƒê°í•´ ë³¼ í•„ìš”ê°€ ìˆìŠµë‹ˆë‹¤.\n",
      "ì§€ê¸ˆê¹Œì§€ ì‚´í´ë³¸ ë°”ì™€ ê°™ì´ ì—¬ëŸ¬ ê°€ì§€ ì¸¡ë©´ì—ì„œ ë³´ë©´ ìš°ì„ \n"
     ]
    }
   ],
   "source": [
    "text2 = 'í˜„ëŒ€ì¸ë“¤ì€ ì™œ í•­ìƒ ë¶ˆì•ˆí•´ í• ê¹Œ?'\n",
    "\n",
    "input_ids2 = tokenizer.encode(text2, return_tensors='pt')\n",
    "gen_ids2 = model.generate(input_ids2,\n",
    "                           max_length=128,\n",
    "                           repetition_penalty=2.0,\n",
    "                           pad_token_id=tokenizer.pad_token_id,\n",
    "                           eos_token_id=tokenizer.eos_token_id,\n",
    "                           bos_token_id=tokenizer.bos_token_id,\n",
    "                           use_cache=True)\n",
    "\n",
    "generated2 = tokenizer.decode(gen_ids2[0])\n",
    "print(generated2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt_env",
   "language": "python",
   "name": "gpt_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
